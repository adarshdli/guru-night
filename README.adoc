= Red Hat Developers Workshop Guide
:experimental:

How to setup the workshop using OpenShift4 

== Pre-req

The scripts uses these tools for various functions, please have them installed before proceeding further in the setup.

- https://github.com/mikefarah/yq[yq]
- https://stedolan.github.io/jq/[jq]
- https://github.com/che-incubator/chectl[chectl]

== OCP4 Setup

=== RHPDS

Setup OpenShift4 Cluster via RHPDS 

=== Bring Your Own Cluster (BYC)
Create a OCP4 cluster by visiting try.openshift.com and downloading the openshift installer binary for Linux or Mac

1) ./openshift-install --dir=june22v2 create install-config

Follow the prompts for your SSH key, your favorite AWS Region, your Route53 domain, your PullSecret (visible at try.openshift.com), etc. This creates a directory called "june22v2" and inside it a file called install-config.yaml

2) Edit this file to override your aws/azure/gcp Instance Types (the default ones are tiny), here is an example snippet

----
  name: worker
  platform: 
     aws:
       type: m5.4xlarge

  name: master
  platform: 
     aws:
       type: c5.4xlarge
----

3) ./openshift-install --dir=june22v2 create cluster

4) You can monitoring the activity by refreshing your AWS EC2 console

5) Make note of the resulting URLs and the kubeadmin password when the process is completed.  It will take several minutes, go have lunch.

6) Also keep the kubeconfig file in june22v2/auth

7) OpenShift4 CLI for your environment from https://mirror.openshift.com/pub/openshift-v4/clients/ocp/latest/.

== Assumption

Its assumed that the cluster has been setup in one of the ways as above.

== Workshop setup

=== Set Environment Variables
Copy `setEnv.example` to `setEnv.sh`. Edit `setEnv.sh` and update the variables with values to match your environment. Login in as OpenShift cluster admin

[source,bash,subs="attributes+,+macros]
----
oc login -u OPENSHIFT_ADMIN_USER -p $OPENSHIFT_ADMIN_USER_PASSWORD $OPENSHIFT_API_URL
----

== RHPDS

=== Delete quotas

[IMPORTANT]
=====
**ONLY FOR RHPDS**
The RHPDS by default applies a `ClusterResourceQuota` to the cluster, you need to remove the `ClusterResourceQuota` or alter to allow extra CPU/RAM for successful Istio install
=====

[source,bash,subs="attributes+,+macros"]
----
# (optional) take a the back up of listed clusterresource quotas
oc get clusterresourcequotas.quota.openshift.io clusterquota-opentlc-mgr -o yaml > <your-backup-dir>/clusterquota-opentlc-mgr.yaml
# scale down the operator
oc -n gpte-userquota-operator scale deployments --replicas=0 userquota-operator
# delete the mulitproject clusterresource quota
oc delete clusterresourcequotas.quota.openshift.io clusterquota-opentlc-mgr
# check if its deleted
oc get clusterresourcequotas.quota.openshift.io
----

=== Install Nexus

Since there will be lot java artifacts required 

[source,bash,subs="attributes+,+macros"]
----
oc adm new-project rhd-workshop-infra
oc new-app -n rhd-workshop-infra sonatype/nexus
----

=== Create Workshop Users

[NOTE]
=====
If you are using RHPDS then the users are already created, hence you skip this step
=====

[source,bash]
----
./workshopper createUsers
----

=== Install Operators

All the components such as Knative Serving, Knative Eventing, Eclipse Che and Tekton will be installed using operators via https://opeatorhub.io[Opeator Hub]. As a first step towards that we need to install the sources from where the components will be installed

[source,bash,subs="attributes+,+macros]
----
./workshopper installCatalogSources
----

[NOTE]
====
It will take few minutes to get the operator sources to be configured
====

Please check if your Operator Catalog looks like below before proceeding to next steps:

Navigate to project `openshift-operators` and navigate to menu:Catalog[Operator Management > Operator Catalogs]

image::./screenshots/operator_catalogs.png[]

=== Eclipse Che

[source,bash,subs="attributes+,+macros]
----
oc adm new-project che --description="Eclipse Che"
./workshopper installChe
----

[NOTE] 
====
It will take few minutes for the Che operator to appear, please wait for it before doing the next step. You can watch the status of the Eclipse Che operator via 
[source,bash,subs="attributes+,+macros]
----
oc get pods -n che -w
----
====

Navigate to project `che` and navigate to menu:Home[Projects > che], please wait for some time for the operator to get installed, a successful Eclipse Che Operator install will be like:

image::./screenshots/eclipse_che_operator_success.png[]

[source,bash,subs="attributes+,+macros]
----
./workshopper createCheCluster
----

It will take few mins for Eclipse Che and its components to be running, you can check the status using the command `oc get -n che pods -w `

A successful Eclipse Che install will have the following components

image::./screenshots/eclipse_che_install_success.png[]

==== Workshop Che optimizations

[source,bash,subs="attributes+,+macros]
----
cat ./config/che.env | oc set env -e - deployment/che -n che
----

====== Get Keycloak Password

[source,bash,subs="attributes+,+macros]
----
export KEYCLOAK_PASSWORD=$(oc get -n che deployment keycloak -o jsonpath='{.spec.template.spec.containers[*].env[?(@.name=="KEYCLOAK_PASSWORD")].value}')
----

====== Get Eclipse Che URL

[source,bash,subs="attributes+,+macros]
----
oc get routes -n che che -o yaml | yq r - 'spec.host'
----

====== Get Keycloak URL

[source,bash,subs="attributes+,+macros]
----
oc get routes -n che keycloak -o yaml | yq r - 'spec.host'
----

The following section are optional based on what components that might be needed for the workshop.

=== Install Istio

Istio will will be installed using Red Hat Servicemesh Operator, the following section details on how to install using operator and oc CLI.

[source,bash,subs="attributes+,+macros"]
----
./workshopper installServicemesh
----

It will take for some time for Servicemesh and its dependencies to be resolve, you can watch the status via:

[source,bash,subs="attributes+,+macros"]
----
watch 'oc get csv -n openshift-operators'
----

A successful install will show an output like 

[source,bash]
----
NAME                                         DISPLAY                          VERSION
           REPLACES   PHASE
elasticsearch-operator.4.1.18-201909201915   Elasticsearch Operator           4.1.18-2019
09201915              Succeeded
jaeger-operator.v1.13.1                      Jaeger Operator                  1.13.1
                      Succeeded
kiali-operator.v1.0.5                        Kiali Operator                   1.0.5
                      Succeeded
servicemeshoperator.v1.0.0                   Red Hat OpenShift Service Mesh   1.0.0
                      Succeeded
----

IMPORTANT: Operator versions may vary from your output based on the latest available csv

===== Deploy Control Plane

[source,bash,subs="attributes+,+macros"]
----
./workshopper createServicemesh
----

NOTE: It will take sometime for Istio to be deployed completely, wait for all the Istio Pods to be available:
[source,bash,subs="attributes+,+macros"]
----
oc -n istio-system get pods -w
----

[NOTE]
====
If you see no pods getting created for long time, try running the command to `oc get svc -n istio-system` to see if the Servicemesh operators are copied to `istio-system` namespace. If you see them copied or copying wait for the copy to be done and run the command `./workshopper createServicemesh` again.
====

==== Get all Service URLS of Istio Services

[source,bash,subs="attributes+,+macros]
----
oc get routes -n istio-system -o custom-columns='NAME:.metadata.name,URL:.spec.host'
----

==== Create Istio service member roll
----
./workshopper addProjectsToServicemesh
----

===== Verify projects added Istio service member roll
----
oc get -n istio-system ServiceMeshMemberRoll -o yaml
----

=== Install Knative

We will be using Knative Serving and Knative Eventing Operators to install Knative Serving and Eventing components:

==== Knative Serving

[source,bash,subs="attributes+,+macros]
----
./workshopper installKnativeServing
----

[NOTE]
====
It will take few minutes for the Knative serving pods to appear please run the following commands to watch the status:
[source,bash,subs="attributes+,+macros]
----
oc -n knative-serving get pods -w
----
====

==== Knative Eventing

[source,bash,subs="attributes+,+macros]
----
./workshopper installKnativeEventing
----

[NOTE]
====
It will take few minutes for the Knative eventing pods to appear please run the following commands to watch the status:
[source,bash,subs="attributes+,+macros]
----
oc -n knative-eventing get pods -w
----
====

=== Strimzi Kafka

[source,bash,subs="attributes+,+macros]
----
./workshopper installKafka
----

[NOTE]
====
It will take few minutes for the Kafka pods to appear please run the following commands to watch the status:
[source,bash,subs="attributes+,+macros]
----
oc -n kafka get pods -w
----
====

=== Knative Eventing Kafka

[source,bash,subs="attributes+,+macros]
----
./workshopper installKnativeKafka
----

[NOTE]
====
It will take few minutes for the Knative Eventing Kafka pods to appear please run the following commands to watch the status:
[source,bash,subs="attributes+,+macros]
----
oc -n knative-eventing get pods -w
----
====

=== OpenShift Pipelines

[source,bash,subs="attributes+,+macros]
----
./workshopper installPipelines
----

[NOTE]
====
It will take few minutes for the OpenShift pipelines pods to appear please run the following commands to watch the status:
[source,bash,subs="attributes+,+macros]
----
oc -n openshift-pipelines get pods -w
----
====

== Workshop users, projects and quotas

=== Cache frequently used container images

[source,bash,subs="attributes+,+macros]
----
./workshopper cacheImages
----

=== Create Workshop User Group and Role
[source,bash]
-----
./workshopper usersAndGroups
-----

You can check the group users via command, which should basically list all workshop users.

[source,bash]
----
oc get groups workshop-students
----

=== Create Eclipse Che Users

[source,bash]
-----
./workshopper createCheUsers
-----

=== Create Eclipse Che User Workspaces

[source,bash]
-----
./workshopper createWorkspaces
-----

It will take sometime to create the workspaces, all the workspaces created will be logged in `$PROJECT_HOME/workspace.txt` file.

=== Create and configure Workshop Projects

==== Create project

[source,bash]
-----
./workshopper configProjects
-----

==== Annotate project

Annotate the projects with right OpenShift user to allow resource quotas to be applied

[source,bash]
-----
./workshopper annotateProjects
-----

== Cleanup

[source,bash]
-----
./workshopper cleanup
-----